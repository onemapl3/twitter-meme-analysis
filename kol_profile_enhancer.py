#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
KOLæ¡£æ¡ˆå¢å¼ºç³»ç»Ÿ - ä»»åŠ¡2å®ç°
æ•°æ®æºæ‰©å±•ä¸æ•´åˆ
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import re
from collections import defaultdict
import json
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class KOLProfileEnhancer:
    def __init__(self):
        """åˆå§‹åŒ–KOLæ¡£æ¡ˆå¢å¼ºå™¨"""
        self.enhanced_profiles = {}
        self.domain_keywords = {
            'crypto': ['bitcoin', 'ethereum', 'nft', 'defi', 'blockchain', 'crypto', 'btc', 'eth', 'token', 'wallet'],
            'tech': ['ai', 'machine learning', 'startup', 'tech', 'innovation', 'software', 'coding', 'programming'],
            'finance': ['trading', 'investment', 'stocks', 'finance', 'economy', 'market', 'portfolio', 'wealth'],
            'entertainment': ['gaming', 'art', 'music', 'film', 'celebrities', 'entertainment', 'culture'],
            'politics': ['politics', 'government', 'policy', 'election', 'democracy', 'society']
        }
        
    def enhance_kol_profiles(self, kol_data, tweets_df, followings_df):
        """å¢å¼ºKOLæ¡£æ¡ˆä¿¡æ¯"""
        print("å¼€å§‹å¢å¼ºKOLæ¡£æ¡ˆ...")
        
        for user_id, kol_info in kol_data.items():
            print(f"å¤„ç†ç”¨æˆ·: {kol_info['user_name']}")
            
            # è·å–ç”¨æˆ·æ¨æ–‡
            user_tweets = tweets_df[tweets_df['user_id'] == user_id]
            
            # 1. ä¸“ä¸šé¢†åŸŸè¯†åˆ«
            domain_info = self._identify_domain(user_tweets, kol_info, followings_df)
            
            # 2. å½±å“åŠ›æ—¶é—´åºåˆ—
            influence_timeline = self._analyze_influence_timeline(user_tweets, kol_info)
            
            # 3. å†…å®¹ç‰¹å¾æå–
            content_features = self._extract_content_features(user_tweets)
            
            # æ•´åˆå¢å¼ºä¿¡æ¯
            self.enhanced_profiles[user_id] = {
                **kol_info,
                'enhanced_domain': domain_info,
                'influence_timeline': influence_timeline,
                'content_features': content_features
            }
        
        print(f"å®Œæˆ {len(self.enhanced_profiles)} ä¸ªKOLæ¡£æ¡ˆå¢å¼º")
        return self.enhanced_profiles
    
    def _identify_domain(self, user_tweets, kol_info, followings_df):
        """ä¸“ä¸šé¢†åŸŸè¯†åˆ«ç®—æ³•"""
        domain_scores = defaultdict(float)
        
        # 1. æ–‡æœ¬å…³é”®è¯åˆ†æ
        text_score = self._analyze_text_keywords(user_tweets)
        for domain, score in text_score.items():
            domain_scores[domain] += score * 0.4  # æƒé‡40%
        
        # 2. å…³æ³¨ç”¨æˆ·é¢†åŸŸåˆ†å¸ƒåˆ†æ
        network_score = self._analyze_following_domains(kol_info['user_id'], followings_df)
        for domain, score in network_score.items():
            domain_scores[domain] += score * 0.3  # æƒé‡30%
        
        # 3. æ¨æ–‡ä¸»é¢˜èšç±»åˆ†æ
        cluster_score = self._analyze_tweet_clusters(user_tweets)
        for domain, score in cluster_score.items():
            domain_scores[domain] += score * 0.3  # æƒé‡30%
        
        # ç¡®å®šä¸»è¦é¢†åŸŸ
        primary_domain = max(domain_scores.items(), key=lambda x: x[1])
        
        return {
            'primary_domain': primary_domain[0],
            'domain_scores': dict(domain_scores),
            'confidence': primary_domain[1]
        }
    
    def _analyze_text_keywords(self, user_tweets):
        """æ–‡æœ¬å…³é”®è¯åˆ†æ"""
        domain_scores = defaultdict(float)
        
        if user_tweets.empty:
            return domain_scores
        
        # åˆå¹¶æ‰€æœ‰æ¨æ–‡æ–‡æœ¬
        all_text = ' '.join(user_tweets['text'].fillna('').astype(str))
        all_text = all_text.lower()
        
        # è®¡ç®—æ¯ä¸ªé¢†åŸŸçš„å…³é”®è¯å‡ºç°é¢‘ç‡
        for domain, keywords in self.domain_keywords.items():
            score = 0
            for keyword in keywords:
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å®Œæ•´å•è¯
                pattern = r'\b' + re.escape(keyword) + r'\b'
                matches = len(re.findall(pattern, all_text))
                score += matches
            
            # æ ‡å‡†åŒ–åˆ†æ•°
            if score > 0:
                domain_scores[domain] = min(score / 10, 1.0)  # æœ€é«˜1.0åˆ†
        
        return domain_scores
    
    def _analyze_following_domains(self, user_id, followings_df):
        """å…³æ³¨ç”¨æˆ·é¢†åŸŸåˆ†å¸ƒåˆ†æ"""
        domain_scores = defaultdict(float)
        
        # è·å–ç”¨æˆ·å…³æ³¨çš„äºº
        following_users = followings_df[followings_df['user_id'] == user_id]['following_user_id'].unique()
        
        if len(following_users) == 0:
            return domain_scores
        
        # åˆ†æå…³æ³¨ç”¨æˆ·çš„é¢†åŸŸåˆ†å¸ƒ
        for following_id in following_users:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æŸ¥è¯¢å…³æ³¨ç”¨æˆ·çš„é¢†åŸŸä¿¡æ¯
            # æš‚æ—¶åŸºäºç”¨æˆ·IDçš„å“ˆå¸Œå€¼åˆ†é…é¢†åŸŸ
            domain_index = hash(following_id) % len(self.domain_keywords)
            domain = list(self.domain_keywords.keys())[domain_index]
            domain_scores[domain] += 1
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        total_following = len(following_users)
        for domain in domain_scores:
            domain_scores[domain] = domain_scores[domain] / total_following
        
        return domain_scores
    
    def _analyze_tweet_clusters(self, user_tweets):
        """æ¨æ–‡ä¸»é¢˜èšç±»åˆ†æ"""
        domain_scores = defaultdict(float)
        
        if len(user_tweets) < 5:  # æ¨æ–‡å¤ªå°‘æ— æ³•èšç±»
            return domain_scores
        
        try:
            # æ–‡æœ¬å‘é‡åŒ–
            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            text_vectors = vectorizer.fit_transform(user_tweets['text'].fillna(''))
            
            # K-meansèšç±»
            n_clusters = min(3, len(user_tweets) // 2)
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(text_vectors)
            
            # åˆ†ææ¯ä¸ªèšç±»çš„ä¸»é¢˜
            for cluster_id in range(n_clusters):
                cluster_tweets = user_tweets[clusters == cluster_id]
                cluster_text = ' '.join(cluster_tweets['text'].fillna('').astype(str))
                
                # è®¡ç®—èšç±»ä¸å„é¢†åŸŸçš„ç›¸å…³æ€§
                for domain, keywords in self.domain_keywords.items():
                    score = 0
                    for keyword in keywords:
                        pattern = r'\b' + re.escape(keyword) + r'\b'
                        matches = len(re.findall(pattern, cluster_text.lower()))
                        score += matches
                    
                    if score > 0:
                        domain_scores[domain] += score / n_clusters
            
            # æ ‡å‡†åŒ–åˆ†æ•°
            if domain_scores:
                max_score = max(domain_scores.values())
                for domain in domain_scores:
                    domain_scores[domain] = min(domain_scores[domain] / max_score, 1.0)
                    
        except Exception as e:
            print(f"èšç±»åˆ†æå¤±è´¥: {e}")
        
        return domain_scores
    
    def _analyze_influence_timeline(self, user_tweets, kol_info):
        """å½±å“åŠ›æ—¶é—´åºåˆ—åˆ†æ"""
        if user_tweets.empty:
            return {}
        
        # æŒ‰æ—¶é—´æ’åº
        user_tweets = user_tweets.sort_values('created_at')
        
        # æ—¶é—´çª—å£åˆ’åˆ† (7å¤©ã€30å¤©ã€90å¤©)
        time_windows = [7, 30, 90]
        timeline_data = {}
        
        for window_days in time_windows:
            window_data = self._calculate_window_influence(user_tweets, window_days)
            timeline_data[f'{window_days}d'] = window_data
        
        # è¶‹åŠ¿åˆ†æ
        trend_analysis = self._analyze_influence_trend(timeline_data)
        
        return {
            'windows': timeline_data,
            'trend': trend_analysis
        }
    
    def _calculate_window_influence(self, user_tweets, window_days):
        """è®¡ç®—æ—¶é—´çª—å£å†…çš„å½±å“åŠ›"""
        if user_tweets.empty:
            return {}
        
        # è·å–æ—¶é—´èŒƒå›´
        end_time = user_tweets['created_at'].max()
        start_time = end_time - timedelta(days=window_days)
        
        # ç­›é€‰æ—¶é—´çª—å£å†…çš„æ¨æ–‡
        window_tweets = user_tweets[user_tweets['created_at'] >= start_time]
        
        if window_tweets.empty:
            return {
                'tweet_count': 0,
                'total_engagement': 0,
                'avg_engagement': 0,
                'influence_score': 0
            }
        
        # è®¡ç®—å½±å“åŠ›æŒ‡æ ‡
        tweet_count = len(window_tweets)
        total_engagement = (window_tweets['likes'].sum() + 
                          window_tweets['retweets'].sum() + 
                          window_tweets['replies'].sum())
        avg_engagement = total_engagement / tweet_count if tweet_count > 0 else 0
        
        # å½±å“åŠ›åˆ†æ•°è®¡ç®—
        influence_score = self._calculate_window_influence_score(
            tweet_count, total_engagement, avg_engagement, user_tweets.iloc[0] if not user_tweets.empty else {}
        )
        
        return {
            'tweet_count': tweet_count,
            'total_engagement': total_engagement,
            'avg_engagement': avg_engagement,
            'influence_score': influence_score
        }
    
    def _calculate_window_influence_score(self, tweet_count, total_engagement, avg_engagement, kol_info):
        """è®¡ç®—æ—¶é—´çª—å£å½±å“åŠ›åˆ†æ•°"""
        # åŸºç¡€å½±å“åŠ› = f(æ¨æ–‡æ•°é‡, äº’åŠ¨é‡, ç”¨æˆ·åŸºç¡€å½±å“åŠ›)
        base_influence = (tweet_count * 0.3 + 
                         total_engagement * 0.4 + 
                         kol_info.get('influence_score', 0) * 0.3)
        
        # æ—¶é—´è¡°å‡å› å­ (è¶Šè¿‘æœŸçš„æ•°æ®æƒé‡è¶Šé«˜)
        time_decay = 1.0  # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥åŸºäºæ—¶é—´å·®è®¡ç®—
        
        return round(base_influence * time_decay, 2)
    
    def _analyze_influence_trend(self, timeline_data):
        """åˆ†æå½±å“åŠ›è¶‹åŠ¿"""
        if not timeline_data:
            return {}
        
        # æå–7å¤©å’Œ30å¤©çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒ
        week_data = timeline_data.get('7d', {})
        month_data = timeline_data.get('30d', {})
        
        if not week_data or not month_data:
            return {'trend': 'insufficient_data'}
        
        # è®¡ç®—è¶‹åŠ¿
        week_influence = week_data.get('influence_score', 0)
        month_influence = month_data.get('influence_score', 0)
        
        if month_influence == 0:
            trend = 'stable'
        else:
            change_rate = (week_influence - month_influence) / month_influence
            if change_rate > 0.1:
                trend = 'increasing'
            elif change_rate < -0.1:
                trend = 'decreasing'
            else:
                trend = 'stable'
        
        return {
            'trend': trend,
            'week_influence': week_influence,
            'month_influence': month_influence,
            'change_rate': round((week_influence - month_influence) / month_influence, 3) if month_influence > 0 else 0
        }
    
    def _extract_content_features(self, user_tweets):
        """å†…å®¹ç‰¹å¾æå–"""
        if user_tweets.empty:
            return {}
        
        # è¯­è¨€é£æ ¼åˆ†æ
        language_style = self._analyze_language_style(user_tweets)
        
        # å†…å®¹ä¸»é¢˜åˆ†å¸ƒ
        topic_distribution = self._analyze_topic_distribution(user_tweets)
        
        # äº’åŠ¨æ¨¡å¼åˆ†æ
        interaction_patterns = self._analyze_interaction_patterns(user_tweets)
        
        return {
            'language_style': language_style,
            'topic_distribution': topic_distribution,
            'interaction_patterns': interaction_patterns
        }
    
    def _analyze_language_style(self, user_tweets):
        """è¯­è¨€é£æ ¼åˆ†æ"""
        if user_tweets.empty:
            return {}
        
        # è®¡ç®—æ¨æ–‡é•¿åº¦ç»Ÿè®¡
        tweet_lengths = user_tweets['text'].str.len()
        
        # æ£€æµ‹æŠ€æœ¯æ€§è¯æ±‡
        technical_terms = ['api', 'algorithm', 'database', 'framework', 'protocol']
        technical_count = sum(user_tweets['text'].str.contains('|'.join(technical_terms), case=False, na=False))
        
        # æ£€æµ‹è¡¨æƒ…ç¬¦å·ä½¿ç”¨
        emoji_pattern = r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]'
        emoji_count = sum(user_tweets['text'].str.count(emoji_pattern))
        
        return {
            'avg_length': round(tweet_lengths.mean(), 1),
            'length_variance': round(tweet_lengths.var(), 1),
            'technical_ratio': round(technical_count / len(user_tweets), 3),
            'emoji_ratio': round(emoji_count / len(user_tweets), 3),
            'style_type': 'technical' if technical_count > len(user_tweets) * 0.3 else 'casual'
        }
    
    def _analyze_topic_distribution(self, user_tweets):
        """å†…å®¹ä¸»é¢˜åˆ†å¸ƒåˆ†æ"""
        if user_tweets.empty:
            return {}
        
        # ç®€å•çš„ä¸»é¢˜å…³é”®è¯ç»Ÿè®¡
        topic_keywords = {
            'business': ['business', 'company', 'startup', 'entrepreneur'],
            'technology': ['tech', 'software', 'app', 'platform'],
            'finance': ['money', 'investment', 'trading', 'profit'],
            'social': ['people', 'community', 'social', 'friends']
        }
        
        topic_counts = defaultdict(int)
        total_tweets = len(user_tweets)
        
        for topic, keywords in topic_keywords.items():
            count = sum(user_tweets['text'].str.contains('|'.join(keywords), case=False, na=False))
            topic_counts[topic] = round(count / total_tweets, 3)
        
        return dict(topic_counts)
    
    def _analyze_interaction_patterns(self, user_tweets):
        """äº’åŠ¨æ¨¡å¼åˆ†æ"""
        if user_tweets.empty:
            return {}
        
        # è®¡ç®—å„ç§äº’åŠ¨ç‡
        total_tweets = len(user_tweets)
        reply_rate = (user_tweets['is_reply'].sum() / total_tweets) if total_tweets > 0 else 0
        quote_rate = (user_tweets['is_quote'].sum() / total_tweets) if total_tweets > 0 else 0
        retweet_rate = (user_tweets['is_retweet'].sum() / total_tweets) if total_tweets > 0 else 0
        
        # è®¡ç®—å¹³å‡äº’åŠ¨é‡
        avg_likes = user_tweets['likes'].mean()
        avg_retweets = user_tweets['retweets'].mean()
        avg_replies = user_tweets['replies'].mean()
        
        return {
            'reply_rate': round(reply_rate, 3),
            'quote_rate': round(quote_rate, 3),
            'retweet_rate': round(retweet_rate, 3),
            'avg_likes': round(avg_likes, 1),
            'avg_retweets': round(avg_retweets, 1),
            'avg_replies': round(avg_replies, 1),
            'interaction_type': 'reactive' if reply_rate > 0.5 else 'proactive'
        }
    
    def save_enhanced_profiles(self, filename='enhanced_kol_profiles.json'):
        """ä¿å­˜å¢å¼ºåçš„KOLæ¡£æ¡ˆ"""
        print(f"ä¿å­˜å¢å¼ºæ¡£æ¡ˆåˆ° {filename}...")
        
        save_data = {
            'enhancement_timestamp': datetime.now().isoformat(),
            'total_profiles': len(self.enhanced_profiles),
            'enhanced_profiles': {}
        }
        
        for user_id, profile in self.enhanced_profiles.items():
            save_data['enhanced_profiles'][user_id] = {
                'user_name': profile['user_name'],
                'influence_score': profile['influence_score'],
                'enhanced_domain': profile['enhanced_domain'],
                'influence_timeline': profile['influence_timeline'],
                'content_features': profile['content_features']
            }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"å¢å¼ºæ¡£æ¡ˆå·²ä¿å­˜åˆ° {filename}")

def main():
    """ä¸»å‡½æ•°"""
    print("=== KOLæ¡£æ¡ˆå¢å¼ºç³»ç»Ÿ ===")
    print("ä»»åŠ¡2: æ•°æ®æºæ‰©å±•ä¸æ•´åˆ")
    print()
    
    # åŠ è½½ä¹‹å‰çš„KOLåˆ†æç»“æœ
    try:
        with open('kol_analysis_results.json', 'r', encoding='utf-8') as f:
            kol_results = json.load(f)
        
        # åˆ›å»ºå¢å¼ºå™¨
        enhancer = KOLProfileEnhancer()
        
        # åŠ è½½æ•°æ®
        print("åŠ è½½æ•°æ®...")
        tweets_df = pd.read_csv('sample_tweets.csv')
        followings_df = pd.read_csv('sample_followings.csv')
        
        # æ•°æ®é¢„å¤„ç†
        tweets_df['created_at'] = pd.to_datetime(tweets_df['created_at'], unit='s')
        tweets_df['user_id'] = tweets_df['user_id'].astype(str)
        followings_df['user_id'] = followings_df['user_id'].astype(str)
        followings_df['following_user_id'] = followings_df['following_user_id'].astype(str)
        
        # è·å–KOLæ•°æ®
        kol_data = {}
        for kol in kol_results.get('top_kols', [])[:20]:  # å¤„ç†å‰20ä¸ªKOL
            user_id = kol['user_id']
            kol_data[user_id] = kol
        
        # å¢å¼ºKOLæ¡£æ¡ˆ
        enhanced_profiles = enhancer.enhance_kol_profiles(kol_data, tweets_df, followings_df)
        
        # ä¿å­˜ç»“æœ
        enhancer.save_enhanced_profiles()
        
        print("\n=== æ¡£æ¡ˆå¢å¼ºå®Œæˆ ===")
        print(f"å¤„ç†KOLæ•°é‡: {len(enhanced_profiles)}")
        
        # æ˜¾ç¤ºç¤ºä¾‹ç»“æœ
        if enhanced_profiles:
            sample_user = list(enhanced_profiles.keys())[0]
            sample_profile = enhanced_profiles[sample_user]
            print(f"\nç¤ºä¾‹ - {sample_profile['user_name']}:")
            print(f"ä¸»è¦é¢†åŸŸ: {sample_profile['enhanced_domain']['primary_domain']}")
            print(f"å½±å“åŠ›è¶‹åŠ¿: {sample_profile['influence_timeline']['trend']['trend']}")
            print(f"è¯­è¨€é£æ ¼: {sample_profile['content_features']['language_style']['style_type']}")
        
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
